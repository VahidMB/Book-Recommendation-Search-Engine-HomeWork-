{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #use for handle requests\n",
    "import time #use for set sleep time\n",
    "from bs4 import BeautifulSoup #use for parse html files\n",
    "import string #use for punctuation\n",
    "import json #use for write & read json files\n",
    "import pandas as pd #use for create dataframe\n",
    "import nltk\n",
    "from nltk.corpus import stopwords #use for detect stopwords in string\n",
    "from nltk.tokenize import word_tokenize #use for getting words in string\n",
    "from nltk.stem import PorterStemmer #use for stemmer of the word\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_url_list = []\n",
    "url_format = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"\n",
    "for i in range(300):\n",
    "    url = url_format + str(i+1)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    first_soup = soup.find_all(\"div\", class_=\"js-tooltipTrigger tooltipTrigger\")\n",
    "    for item in first_soup:\n",
    "        second_soup = BeautifulSoup(str(item), \"html.parser\")\n",
    "        book_link_tag = str(second_soup.find_all(\"a\"))\n",
    "        first_split = book_link_tag.split(\">\")\n",
    "        book_just_links = first_split[0]\n",
    "        second_split = book_just_links.split(\"href=\")\n",
    "        book_url_with_title = second_split[1]\n",
    "        third_split = book_url_with_title.split(\" title\")\n",
    "        book_url_with_qoutation = third_split[0]\n",
    "        book_url = \"https://www.goodreads.com\" + book_url_with_qoutation.strip('\"')\n",
    "        book_url_list.append(book_url)\n",
    "with open(\"book url file.txt\", \"w\") as book_url_file:\n",
    "    json.dump(book_url_list, book_url_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"book url file.txt\", \"r\") as book_url_file:\n",
    "    book_urls = json.load(book_url_file)\n",
    "html_file_path_format = \"/Users/vahidmohammadi/™️/DS/Practice/project/07/HTML Files/page \"\n",
    "for counter_html in range(28020,len(book_urls)):\n",
    "    book = book_urls[counter_html]\n",
    "    try:\n",
    "        response = requests.get(book)\n",
    "        text = response.text\n",
    "        html_file_path = html_file_path_format + str(counter_html + 1) + \".html\"\n",
    "        with open(html_file_path, \"w\") as book_html_file:\n",
    "            book_html_file.write(text)\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"book url file.txt\", \"r\") as book_url_file:\n",
    "    book_urls = json.load(book_url_file)\n",
    "html_file_path_format = \"/Users/vahidmohammadi/™️/DS/Practice/project/07/HTML Files/page \"\n",
    "tsv_file_path_format = \"/Users/vahidmohammadi/™️/DS/Practice/project/07/TSV File/page \"\n",
    "for counter_tsv in range(len(book_urls)):\n",
    "    try:\n",
    "        html_file_path = html_file_path_format + str(counter_tsv + 1) + \".html\"\n",
    "        with open(html_file_path, \"r\") as book_html_file:\n",
    "            content = book_html_file.read()\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        try:\n",
    "            title = soup.find_all(\"h1\", id=\"bookTitle\")[0].text.strip(\"\\n\").strip()\n",
    "        except:\n",
    "            title = \"\"\n",
    "        try:\n",
    "            author = soup.find_all(\"a\", class_=\"authorName\")[0].text.strip(\"\\n\").strip()\n",
    "        except:\n",
    "            author = \"\"\n",
    "        try:\n",
    "            rating_star = soup.find_all(\"span\", itemprop=\"ratingValue\")[0].text.strip(\"\\n\").strip()\n",
    "        except:\n",
    "            rating_star = \"\"\n",
    "        try:\n",
    "            rating_counts = soup.find_all(\"meta\", itemprop=\"ratingCount\")[0].get(\"content\", None)\n",
    "        except:\n",
    "            rating_counts = \"\"\n",
    "        try:\n",
    "            review_counts = soup.find_all(\"meta\", itemprop=\"reviewCount\")[0].get(\"content\", None)\n",
    "        except:\n",
    "            review_counts = \"\"\n",
    "        try:\n",
    "            plot = soup.find_all(\"div\", id=\"description\")[0].text.strip(\"\\n\").replace(\"\\n\",\"\")\n",
    "            if detect(plot) != \"en\":\n",
    "                continue\n",
    "        except:\n",
    "            plot = \"\"\n",
    "        try:\n",
    "            number_of_pages = soup.find_all(\"div\", id=\"details\")[0].find_all(\"span\", itemprop=\"numberOfPages\")[0].text\n",
    "        except:\n",
    "            number_of_pages = \"\"\n",
    "        try:\n",
    "            date = soup.find_all(\"div\", id=\"details\")[0].find_all(\"div\", class_=\"row\")[1].text.strip().split(\"\\n\")[1].strip().strip(\"\\n\")\n",
    "        except:\n",
    "            date = \"\"\n",
    "        try:\n",
    "            info_box = soup.find_all(\"div\", id=\"details\")[0].find_all(\"div\", class_=[\"infoBoxRowTitle\", \"infoBoxRowItem\"])\n",
    "            for i in range(len(info_box)):\n",
    "                try:\n",
    "                    if info_box[i].text == \"Characters\":\n",
    "                        characters = info_box[i+1].text.strip(\"\\n\")\n",
    "                except:\n",
    "                    characters = \"\"\n",
    "                try:\n",
    "                    if info_box[i].text == \"Setting\":\n",
    "                        setting = info_box[i+1].text.replace(\"\\n\", \" \")\n",
    "                except:\n",
    "                    setting = \"\"\n",
    "                try:\n",
    "                    if info_box[i].text == \"Series\":\n",
    "                        series = info_box[i+1].text\n",
    "                except:\n",
    "                    series = \"\"\n",
    "        except:\n",
    "            characters = \"\"\n",
    "            setting = \"\"\n",
    "            series = \"\"\n",
    "        tsv_file_format = title+\"\\t\"+series+\"\\t\"+author+\"\\t\"+rating_star+\"\\t\"+rating_counts+\"\\t\"+review_counts\\\n",
    "        +\"\\t\"+plot+\"\\t\"+number_of_pages+\"\\t\"+date+\"\\t\"+characters+\"\\t\"+setting\n",
    "        tsv_file_path = tsv_file_path_format + str(counter_tsv + 1) + \".tsv\"\n",
    "        with open(tsv_file_path, \"w\") as book_tsv_file:\n",
    "            book_tsv_file.write(tsv_file_format)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this part is used for removing stopwords punctuation and get the stem words in tsv files and\n",
    "#indexing each new words and create dictionary of words exist on movie's tsv file\n",
    "with open(\"book url file.txt\", \"r\") as book_url_file:\n",
    "    book_urls = json.load(book_url_file)\n",
    "words = {} #for indexing each word\n",
    "books_words = {} #for existance of each word in movie's tsv file\n",
    "word_id = 0\n",
    "counter_nltk = 0\n",
    "exclude = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer() #use for finding stem of each word\n",
    "\n",
    "for counter_nltk in range(len(book_urls)):\n",
    "    try:\n",
    "        file_name_tsv = \"/Users/vahidmohammadi/™️/DS/Practice/project/07/TSV File/page \" + str(counter_nltk+1)+\".tsv\"#tsv file path\n",
    "        with open(file_name_tsv, \"r\") as book_tsv_file:#open tsv files\n",
    "            text_list = book_tsv_file.read()\n",
    "            text = \" \".join(text_list.split(\"\\t\")[1:3])\n",
    "            text = ''.join(ch for ch in text if ch not in exclude) #use for getting each word without white spaces\n",
    "            word_tokens = word_tokenize(text)\n",
    "            filtered_sentence = [ps.stem(w) for w in word_tokens if not w in stop_words]#removing stopwords and finding stem of the each word\n",
    "            for word in filtered_sentence:\n",
    "                if word not in words.values():#looking for NOT existance of word in words list for indexing\n",
    "                    words[word_id] = word\n",
    "                    books_words[word_id] = [counter_nltk+1]#adding movies name in movies_words\n",
    "                    word_id += 1\n",
    "                else: # if word exist in words list just add movies ID (movies file name that produce by counter) in movies word\n",
    "                    exist_id = [key for key, value in words.items() if value == word][0]\n",
    "                    if counter_nltk+1 not in books_words[exist_id]:\n",
    "                        books_words[exist_id].append(counter_nltk+1)\n",
    "    except:\n",
    "        continue\n",
    "with open(\"words_index.json\", \"w\") as word_index: #write words list in json file\n",
    "    json.dump(words, word_index)\n",
    "with open(\"books_words.json\", \"w\") as books_words_file:#write movies_words list in json file\n",
    "    json.dump(books_words, books_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "|Which book would you recomend?|\n",
      "================================\n",
      "--------------------\n",
      "|Search Engine v1.0|\n",
      "--------------------\n",
      "search: love story\n",
      "\n",
      "Result: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>plot</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love Story</td>\n",
       "      <td>Oliver Barrett IV, a wealthy jock from a stuff...</td>\n",
       "      <td>https://www.goodreads.com/book/show/73968.Love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloodsucking Fiends</td>\n",
       "      <td>There is an alternate cover edition here.Jody ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/33454.Bloo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  book                                               plot  \\\n",
       "0           Love Story  Oliver Barrett IV, a wealthy jock from a stuff...   \n",
       "1  Bloodsucking Fiends  There is an alternate cover edition here.Jody ...   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.goodreads.com/book/show/73968.Love...  \n",
       "1  https://www.goodreads.com/book/show/33454.Bloo...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this part is used for searching through json files and find first 5 best match and show them in pandas frame work\n",
    "print(\"================================\")\n",
    "print(\"|Which book would you recomend?|\")\n",
    "print(\"================================\")\n",
    "print(\"--------------------\")\n",
    "print(\"|Search Engine v1.0|\")\n",
    "print(\"--------------------\")\n",
    "our_input = input(\"search: \")\n",
    "exclude = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "our_input = ''.join(ch for ch in our_input if ch not in exclude)\n",
    "word_tokens = word_tokenize(our_input)\n",
    "filtered_sentence = [ps.stem(w) for w in word_tokens if not w in stop_words]\n",
    "words_in_input = []\n",
    "with open(\"words_index.json\", \"r\") as word_index:\n",
    "    words = json.load(word_index)\n",
    "with open(\"books_words.json\", \"r\") as books_words_file:\n",
    "    books_words = json.load(books_words_file)\n",
    "for word in filtered_sentence:\n",
    "    for key, value in words.items():\n",
    "        if word == value:\n",
    "            words_in_input.append(key)\n",
    "words_in_books = []\n",
    "for key_of_input in words_in_input:#finding each searched word's index\n",
    "    for key_of_books, value in books_words.items():\n",
    "        if key_of_input == key_of_books:\n",
    "            words_in_books.append(value)\n",
    "final_result_dict = {}#use for saving matches\n",
    "for first_item in words_in_books:#searching by words index that exist in movies words list\n",
    "    for second_item in first_item:\n",
    "        if second_item in final_result_dict.keys():\n",
    "            final_result_dict[second_item] += 1\n",
    "        else:\n",
    "            final_result_dict[second_item] = 1\n",
    "all_item_match_dict = {}\n",
    "for key, value in final_result_dict.items():\n",
    "    if value == len(words_in_input):\n",
    "        all_item_match_dict[key] = value\n",
    "print(\"\")\n",
    "print(\"Result: \")\n",
    "df = {\"book\":[], \"plot\":[], \"link\":[]}#use for creating pandas dataframe\n",
    "with open(\"book url file.txt\", \"r\") as book_url_file:\n",
    "    books_urls = json.load(book_url_file)\n",
    "for item in all_item_match_dict:\n",
    "    file_name = \"/Users/vahidmohammadi/™️/DS/Practice/project/07/TSV File/page \" + str(item) + \".tsv\"#tsv file path\n",
    "    with open(file_name, \"r\") as target:#open tsv file for reading title and intro\n",
    "        tsv_file = target.read()\n",
    "        book_list = tsv_file.split(\"\\t\")\n",
    "        df[\"book\"].append(book_list[0])\n",
    "        df[\"plot\"].append(book_list[6])\n",
    "        df[\"link\"].append(books_urls[item-1])\n",
    "pd_df = pd.DataFrame(df)\n",
    "if pd_df.empty:\n",
    "    print(\"NOT Found 404\")\n",
    "pd_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
